---
title: "Drawings as a window into developmental changes in object representations"
bibliography: kiddraw.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: >
    \author{{\large \bf Bria Long} \\ \texttt{bria@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Judith E. Fan} \\ \texttt{jefan@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Michael C. Frank } \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract:
    "How do children's representations of object categories change as they grow older? As they learn about the world around them, they also express what they know in the drawings they make. Here, we examine drawings as a window into how children represent familiar object categories, and how this changes across childhood. We asked children (age 3-10 years) to draw objects on an iPad, and analyzed their semantic properties. Across this age range, we found large and consistent gains in how well children could produce drawings that are recognizable to adults. We hypothesized that this improvement reflects increasing convergence between children's representation of object categories and that of adults. To measure this, we extracted features of drawings made by children and adults using a pre-trained deep convolutional neural network, allowing us to visualize the representational layout of object categories across age groups using a common feature basis. We found that the organization of object categories in older children's drawings were more similar to that of adults than younger children's drawings. This correspondence was especially strong for higher layers of the neural network, showing that older children’s drawings tend to capture high-level perceptual features critical for adult recognition. Broadly, these findings point to drawing as a rich source of insight into how children represent object concepts."

keywords:
    "object representations; drawings; child development"

output: cogsci2016::cogsci_paper
---
```{r echo=FALSE, include=FALSE}
# NOTES
# Still working on making the entire notebook reproducible, including porting some of the Jupyter notebook outputs over).
# Some of the figures need improving here - colors are off between two of the age plots
```

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(knitr)
library(tidyverse)
library(assertthat)
library(ggthemes)
library(viridis)
library(lme4)
library(langcog)
library(forcats)
library(egg)
library(gridExtra)
theme_set(theme_few())
```

# Introduction

As humans, we have many powerful tools to externalize what we know, including language and gesture. One tool that has been particularly transformative for human cognition and culture is graphical representation, which allows people to encode their thoughts in a visible, durable format. Drawing is an important case study in graphical representation, being a technique that dates back at least 40,000 years [@pike2012u], well before the emergence of symbolic writing systems [@clottes2008cave], and is practiced in many cultures [@gombrich1989story].

In modern times, drawings are produced prolifically by children from an early age [@kellogg1969analyzing], providing a rich source of potential insight into their emerging understanding of the visual world. For example, as children learn the diagnostic properties of objects they encounter, they might express this knowledge in the drawings they make. How can we leverage this natural behavior to understand how they learn abstractions over their perceptual experience, such as object categories?

We know that children quickly reach a remarkable degree of sophistication in forming abstract perceptual representations of the objects they frequently encounter, leveraging shape information (in conjunction with linguistic cues) to learn about invariant properties of object categories [@smith2002object]. Typically, such learning is measured using discrete choices between stimuli that vary along dimensions chosen by the experimenter; by contrast, drawing tasks permit children to include any information they consider relevant for conveying information about an object. For example, when presented with a target object to draw in which a prominent feature is occluded (e.g., the handle of a mug is turned away), children as young as 5 years of age frequently include the occluded object part in their drawing anyway, displaying the robustness of their internal representation to variation in viewpoint [@davis1983contextual].

We also know that important developmental changes in perceptual processing continue throughout childhood (for reviews, see @nishimura2009; @juttner2016developmental). For example, young children tend to categorize novel objects on the basis of part-specific information, whereas older children additionally recruit information about relationships between object parts [@mash2006]. his is resonant with evidence from the drawings they make: drawings by children between 4-8 years of age reveal rapid changes in how they encode semantically relevant information in their drawings: younger children tend to include fewer cues in their drawings to differentiate between target concepts (e.g., adult vs. child) than older children, who enrich their drawings with more diagnostic part [@sitton1992drawing] or relational [@light1983effects] information, especially when they know their goal is to make their drawings recognizable to someone else. However, while large numbers of discrimination trials are required to yield reliable estimates of perceptual performance along a constrained set of pre-defined stimulus dimensions, even a single drawing provides richly high-dimensional information about the content and structure of children's perceptual representations of semantically relevant information.

While figurative drawing tasks have long provided inspiration for scientists investigating the representation of object concepts in early life [@minsky1972artificial], a major barrier to understanding has been the lack of principled, quantitative measures of high-level semantic information in drawings. As such, previous studies employing drawing tasks have typically relied on qualitative assessments [@kosslyn1977children] or ad hoc quantitative criteria [@goodenough1963goodenough]. Recent work in computational vision has validated the use of pre-trained deep convolutional neural network (DCNN) models to quantitatively measure semantic information in adult drawings [@fan2015common]. Higher layers of these models, in addition to predicting neural population responses in object-selective cortex [@yamins2014performance], capture adult perceptual judgments of object shape similarity [@kubilius2016deep]. Together, this prior work suggests that features learned by these models provide a principled choice of high-level perceptual feature basis for extracting semantic information, including object identity, from children's drawings.

```{r exampleDrawings, fig.env = "figure*", fig.pos = "h", out.width="100%", fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Example drawings made by children ages 4-10 of several object categories."}
img <- png::readPNG("figs/drawings.png")
grid::grid.raster(img)
```

Here we examine children's drawings as a window into how they represent familiar visual object categories, and how this representation changes across childhood. We ask children to draw a variety of object categories on a digital tablet. Then, we examine how semantic information in children's drawings changes with age, after factoring out low-level covariates related to motor production, such as how long they spend drawing and how many strokes they use. Second, we compare the perceptual features of child and adult drawings by comparing their representations in a pre-trained DCNN model, allowing us to visualize the representational layout of object categories across age groups using a common feature basis.

# Part 1: Adult recognition of children's drawings
How do children's ability to convey semantically relevant information in their drawings change across childhood? Our approach was to collect multiple drawings across several object categories across a broad age range. First, children (ages 3-10 years) produced drawings of 16 common object categories in a simple drawing game. Then, other participants (naïve adults) attempted to recognize these drawings in a forced-choice recognition task.

## Methods

### Participants
For the drawing task, children (N = 41, M = 6.9 years, range 4-10 years) were recruited at the San Jose Children’s Discovery Museum. Either the child or their parents verbally reported the child's age. For the recognizability experiment, 14 naïve adults with US IP addresses were recruited from Amazon Mechanical Turk and provided labels for all drawings.

### Stimuli
Stimuli were words referring to 16 common object categories: banana, boat, car, carrot, cat, chair, couch, cup, flower, foot, frog, ice cream, phone, rabbit, shoe, train. These categories were chosen such that they were: (1) likely to be familiar to children, (2) spanned the animate/inanimate distinction, and (3) intuitively spanned a wide range of difficulty (for example, flowers seem easier to draw than couches). We also choose items that were present in the Google QuickDraw database, a large dataset containing drawings made by adults in under 20 seconds, so that we could eventually compare children's drawings with ones made by adults.

### Drawing Task Procedure
We implemented a web-based drawing game in HTML/Javascript using the paper.js library and collected drawings using a touchscreen tablet on the floor of the museum. At the beginning of each session, to familiarize children with the task and touch interface, they were prompted to draw a circle and a triangle. After completing these two practice trials, they were cued to draw a randomly selected object. On each trial, a text cue would appear (i.e., “Can you draw a [flower]?”) that the experimenter would read out, (“What about a [flower]? Can you draw a [flower]?). Then, a drawing canvas appeared (600 x 600 pixels) and children had 30 seconds to make a drawing before moving onto the next trial; pilot testing suggested that 30 seconds was enough for many children to complete their drawings. After each trial, the experimenter asked the child whether they wanted to keep drawing or whether they were all done. In all, we collected 268 drawings across the 16 categories.

### Recognizability Task Procedure
After collecting children's drawings, we presented them to naïve adults to measure their recognizability. On each trial, participants saw a drawing, and were asked “What does this look like?”, and responded by typing their response into a text box. Only labels from a restricted set of 21 options were accepted, comprising the 16 drawn categories, 4 foil categories (bean, arm, person, rock), and "cannot tell at all." Drawings were presented in a random order, and participants were not informed that these drawings were produced by children or the context in which they were produced.

```{r echo=FALSE, include=FALSE}
## Load data and do basic preprocessing.

## Read in data outputs from python - stroke numbers, intensity, bounding boc, etc.
# get rid of drwaings without age - these were when we were testing the interface.
# make new variable name with image name for joining with recognition data
d <- read_csv("e1-preprocessedData/museumdraw_E1c_imageData.csv") %>%
  filter(!is.na(age)) %>%
  mutate(imNameShort = paste0(category, '_sketch', '_', age,'_', session_id, '.png'))

## Read in data outputs from turk data - true/false recognition with 21AFC
r <- read.csv("e1-preprocessedData/museumdraw_E1c_recognitionData.csv") %>%
  as.tibble()

## check we have the right lengths
assert_that(length(d$session_id)==length(unique(r$imageName)))

# add special column for when people selected "can't tell at all" during ratings; not separated out in current analyses
r$cantTell=(r$rating=="cannott tell at all")

## Get the percent recognized for each drawing
corbyItem <- r %>%
  group_by(imNameShort) %>%
  summarize(meanCorrect = mean(correct),
            propCantTell = mean(cantTell))

## Join the two datasets together
joint=left_join(d,corbyItem) %>%
  mutate(session_id = factor(session_id),
         category = factor(category))

## for use below with glmer analyses
joinedRatings <- left_join(r,d)
joinedRatings$session_id<-factor(joinedRatings$session_id)

## percent correct by age
ageCorrOut<-joint %>%
  group_by(age) %>%
  summarize(count = n(),
            meanCorrect = mean(meanCorrect),
            propCantTell = mean(propCantTell))

```

###  Model Fitting
Our goal was to measure how children's ability to convey semantically relevant information in their drawings changes with age. We anticipated that their drawings may also vary along other dimensions more directly related to the motor production demands of the task, such as the amount of time spent drawing, the number of strokes used, and amount of ink (i.e., mean pixel intensity of sketch). See Figure \ref{fig:covDescriptives}.

In order to assess whether children’s ability to produce recognizable drawings increased with age, independent of these low-level covariates, we fit a generalized linear mixed-effects model, with age (specified in years), drawing duration, amount of ink used, and number of strokes as fixed effects, and with random intercepts for each individual child drawer and object category. The dependent variable was whether the proportion of adults that recognized a given drawing. This was specified in the lme4 r package as: glmer(correct ~ scale(age) +  scale(draw_duration) +  scale(mean_intensity) +  scale(num_strokes) +(1|session_id) + (1|category), family = "binomial").

``` {r include=FALSE, echo=FALSE}
## GLMM procedure
modelOut <- glmer(correct ~ age + (1 | session_id) + (1 | category),
      data = joinedRatings,  
      family = "binomial")

mod_covariates <- glmer(correct ~ scale(age) +
                          scale(draw_duration) +
                          scale(mean_intensity) +
                          scale(num_strokes) +
                        (1|session_id) +
                        (1|category),
      data = joinedRatings,  
      family = "binomial")

mod_covariates_2 <- glmer(correct ~ (scale(age) +
                          scale(draw_duration) +
                          scale(num_strokes))^2 +
                        (1|session_id) +
                        (1|category),
      data = joinedRatings,  
      family = "binomial")

modelOut=summary(mod_covariates)
modelOut_Int=summary(mod_covariates_2)
```
```{r recognizabilityByItem, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=3, fig.height=2.5, fig.cap = "Proportion of drawings recognized in each object category. The dashed line represents chance performance. Error bars represent non-parametric 95 percent confidence intervals, estimated using the langcog r package. " }

ms <- joint %>%
  mutate(age_group = cut(age, c(3.9, 6, 10.1), labels = c("3-6","7-10"))) %>%
  group_by(category, age_group) %>%
  multi_boot_standard(col = "meanCorrect")  %>%
  ungroup %>%
  mutate(category = fct_reorder(category, mean))

chance=1/21
ggplot(ms, aes(x = category, y = mean, col = age_group)) +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  coord_flip() +
  theme_bw() +
  labs(y = "Proportion recognized", x = "Object category") +
  scale_color_viridis(discrete=TRUE, begin=0, end=.4, "", labels=c("3-6 yrs.", "7-10 yrs.")) +
  theme(legend.position = c(0.2, 0.8), legend.text = element_text(size=6), legend.background =   element_rect(fill=alpha('white', 0))) +
  geom_hline(yintercept=c(chance), linetype="dotted")

```
## Results


We found that recognizability of drawings generally increased with age (see Figure \ref{fig:covDescriptives}), although there was substantial variability across categories in how well children could produce recognizable drawings. For example, children of all ages produced drawings of cats that were readily recognizable as "cats", but few children of any age produced drawings that were recognizable as "shoes" (see Figure \ref{fig:recognizabilityByItem}).

\begin{table}[H]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\
  \hline
(Intercept) & 0.861 & 0.321 & 2.680 & 0.007 \\
  Age & 0.956 & 0.174 & 5.497 & 0.000 \\
  Drawing time & 0.338 & 0.109 & 3.105 & 0.002 \\
  Amount of ink & 0.014 & 0.080 & 0.179 & 0.858 \\
  Num. strokes & -0.289 & 0.098 & -2.959 & 0.003 \\
   \hline
\end{tabular}
\caption{Model coefficients of a GLMM predicting the recognizability of each  drawing.}
\end{table}

Was this increase due to an increase in high-level semantic properties of older children's drawings, or to the possibility that older children may have put more time and effort into their drawings? To address this question, we fit a generalized linear mixed-effects model that included
various low-level covariates related to time and effort: drawing duration, number of strokes, and amount of ink used.

This analysis revealed that recognizability of drawings reliably increased when controlling for these low-level covariates — the amount of time spent drawing, the number of strokes, and total ink used (b = `r format(modelOut$coefficients[2,1],digits=2)`, SE = `r format(modelOut$coefficients[2,2],digits=2) `, Z = `r format(modelOut$coefficients[2,3],digits=2)`), and accounting for variation across object categories and individual children. All model coefficients can be seen in Table 1. Adding interaction terms between age and these low-level covariates did little to decrease the effect of age on recognizability (b = `r format(modelOut_Int$coefficients[2,1],digits=2)`, SE = `r format(modelOut_Int$coefficients[2,2],digits=2) `, Z = `r format(modelOut_Int$coefficients[2,3],digits=2)`).  

<!-- ```{r modelCoefficients, results="asis"} -->
<!-- tab1 <- xtable::xtable(summary(mod_covariates)$coef, digits=3,  -->
<!--                       caption = "Model coefficients of a GLMM predicting the recognziability of each  drawing.") -->

<!-- print(tab1, type="latex", comment = F, table.placement = "H") -->
<!-- ``` -->


```{r covDescriptives, fig.env="figure*", fig.height=2, fig.width=7, fig.pos = "h", fig.align = "center", fig.cap = "The proportion of adults who recognized each drawing is plotted as a function of child's age, the number of strokes, amount of ink used, and the time spent creating each drawing. Each dot represents an individual drawing; dots in the right three plots are colored by the age of the drawer." }
ms <- joint

p4<-ggplot(joint, aes(age, meanCorrect)) +
  geom_jitter(alpha=.5, width = .1) +
  geom_smooth(method="lm",span=2, alpha=.1,color="orange") +
  theme_few() +
  ylim(0,1) +
  labs(y = "Prop. recognized", x = "Age (years)")

p1<-ggplot(ms, aes(x = num_strokes, y=meanCorrect, color=age)) +
  geom_jitter(alpha=.7) +
  geom_smooth(aes(group = 1), method="lm",span=3, alpha=.1, color="orange") +
  theme_few() +
  ylim(0,1) +
  scale_color_viridis()+
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  theme(legend.position="none") +
  labs(y = "", x = "Number of strokes")

p2<-ggplot(ms, aes(x = mean_intensity, y=meanCorrect, color=age)) +
  geom_jitter(alpha=.7) +
  geom_smooth(aes(group = 1), method="lm",span=3, alpha=.1, color="orange") +
  ylim(0,1) +
  theme_few() +
  scale_color_viridis()+
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  theme(legend.position="none") +
  labs(y = "", x = "'Ink' used")
p3<-ggplot(ms, aes(x = draw_duration, y=meanCorrect, color=age)) +
  geom_jitter(alpha=.7) +
  ylim(0,1) +
  geom_smooth(aes(group = 1), method="lm",span=3, alpha=.1, color="orange") +
  theme_bw() +
  scale_color_viridis()+
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  labs(y = "", x = "Drawing time (s)")

ggarrange(p4,p1,p2,p3, nrow=1)
```
Taken together, these results show large and consistent gains in how well children can produce recognizable drawings across this age range, although younger children still produced drawings that could be recognized well above chance by adult viewers.

# Part 2: Model-based feature analyses of children's drawings

The previous section showed that children's drawings generally contained sufficient semantic information to support recognition by adult viewers, although older children's drawings were consistently more recognizable. What developmental changes to children's object representations might underlie the improved recognizability in older children's drawings (at least to adult viewers)?



While younger children often produced drawings that were unrecognizable at the basic-level, these recognizability ratings may underestimate the perceptual content depicted in children’s drawings. For example, children may not be able to depict the visual differences between a bunny and a frog, but they still may capture many of the essential perceptual features needed to depict an animal. Here, we turn to deep neural network models of object recognition to quantify the similarities between children’s and adults drawings. While the earliest layers of this network tend to capture similarities in low-level features (e.g., spatial frequency, edges), intermediate and higher layers tend to capture similarities in mid- to high-level visual features, including overall object shape and the presence of diagnostic object parts (e.g., legs, handles) [@gucclu2015deep] and predict neural responses in object-selective cortex [@gucclu2015deep; @yamins2014performance].

We thus examine the feature similarity between children and adult's drawings at each layer of a deep convolutional neural network optimized for object recognition--called VGG-19 [@simonyan2014very]. Prior work has found that adults drawings of objects tend to be the most similar to photos of these objects in terms of higher-level layers of deep CNNs [@fan2015common]. Thus, if children's drawings also exhibit high-level perceptual similarities to adults' drawings, then we should expect feature similarity to reach a peak in later, higher layers of the network. However, if children's drawings do not contain these more diagnostic features, then we might expect similarity to peak in earlier convolutional layers. We perform these analyses separately for two age groups, roughly "young children" (ages 3-6) and "older children" (ages 7-10).

## Methods

### Participants
Participants included those who participated in the first round of data collection, as well as an additional 37 children recruited in the same way as in Part 1.

<!-- ### Stimuli
For this analyses, we collected a larger sample of drawings using the same methodology, this time sampling from both the previously used categories as well as a new selection of 22 categories (see Stimuli) allowing us to span superordinate category distinctions and to include equal numbers of vehicles, furniture, small objects, food items, mammals, and non-mammals (new items: airplane, bus, bike, piano, table, door, bed, fork, keys, hat, apple, cookie, mushroom, horse, dog, sheep, bear, fish, bird, spider, shark, duck); this wider category structure has often be used to compare object category representations using deep convolutional neural networks [@yamins2014performance; @kriegeskorte2008RSA]. -->

### Drawing Dataset
In our second round of data collection, our goal was to expand the number of categories included in our model-based feature analyses, so we included an additional 22 categories.

Across both rounds of data collection, we recorded 387 drawings from 78 children across a broad age range. Due to having

For the subsequent analyses, we binned children's age coarsely into "younger children" (aged 3-6 years) and "older" children (aged 7-10 years) and restricted analyses to categories where we had at least 3 drawings per age group (16 categories); this allowed us to analyze approximately the same number of drawings in each age group (younger children, N=118 drawings; older children, N=161 drawings). By including a minimum number of drawings per class and age category, we ensured robust estimates in the following analyses.

### Adult drawings
We obtained a sample of adult drawings from the Google QuickDraw database.
Specifically, we randomly sampled 100 drawings from each object category. See https://quickdraw.withgoogle.com/data for visualizations of this dataset.

### Convolutional Neural Network (CNN) Features  
We used a standard, pre-trained implementation of VGG-19 [@simonyan2014very] to extract features from sketches at layers across several depths in the network. Specifically, we analyzed feature activations in the first five pooling layers, as well as the first two fully-connected layers.

Prior to analysis, we cropped all sketch images to contain only the sketch, applied uniform padding (10px), and rescaled to the same size (3x224x224).

Each image elicited a pattern of feature activations at every layer in the model, each pattern being equivalent to a vector in a feature space with the same number of dimensions as units in that layer.

Features were normalized within-unit across all images, and category-level feature vectors

Category feature vectors were computed by taking the mean across

 across all sketches and then averaged within each category (e.g., “cat”, “rabbit”). This yielded a vector corresponding to the number of features in each layer for all 38 of the drawn categories in younger children, older children, and adults.

```{r RSAAllCat, fig.env = "figure*", fig.pos = "h", fig.align='center', fig.width=7, fig.height=2, set.cap.width=T, num.cols.cap=2, fig.cap = "Representational dissimilarity matrices (RDMs) in the highest layer of VGG-19 (FC7) for drawings made by younger children (3-6 years of age), older children (7-10 years of age), and adults (Google QuickDraw database). Each square in one of these matrices represents the correlation distance between two categories (e.g., chair and couch) in this layer of the network; lighter colors indicate pairs of categories that generated dissimilar feature representations; darker colors indicate pairs of categories that generated more similar feature representations. Categories are grouped to reveal the inherent similarity structure."}
img <- png::readPNG("figs/RSA.png")
grid::grid.raster(img)
```

### Representational Similarity Analyses
Representational similarity analysis treats each feature in VGG-19 as a dimension in a high-dimensional space and uses these values to compute the representational distances between images. Separately for drawings from younger children, older children, and adults, we averaged the feature vectors within each object class for a given layer of VGG and then computed a layer-specific matrix of the Pearson correlation distances between these average vectors across classes [@kriegeskorte2008RSA]. Formally, this entailed computing: $$RDM(R)_{ij} = 1- \frac{cov(\vec{r}_{i}, \vec{r}_{j})}{\sqrt{var(\vec{r}_{i}) \cdot var(\vec{r}_{j})}}$$, where $\vec{r}_{i}$ and $\vec{r}_{j}$ are the mean feature vectors for the $i$th and $j$th object classes, respectively, where R represents the correlation betwen two classes (e.g., rabbits and shoes). Each of these 16x16 representational dissimilarity matrices (RDMs, shown in Figure \ref{fig:RSAAllCat}) provides a compact description of the layout of objects in the high-dimensional feature space inherent to each layer of the model. Following @kriegeskorte2008RSA, we measured the similarity between object representations in different layers by computing the Spearman rank correlations between the RDMs for those corresponding layers. Finally, we computed a noise ceiling by performed the above similarity computations between two randomly sampled subsets of adult drawings that matched the number of drawings children made in each object category; this allowed us to estimate the maximum correlations we expect to observe given the number of drawings we analyze.

Estimates of standard error for the Spearman correlation between RDMs (i.e., between domains or between layers) were generated by jackknife resampling of the 16 object classes. This estimate of standard error allows us to construct 95\% confidence intervals and compute two-sided p-values for specific comparisons [@Efron:1979ts; @Tukey:1958wn]. This entails iterating through each of the 16 subsamples that exclude a single class, computing the correlation on each iteration, then aggregating these values. Specifically, the jackknife estimate of the standard error can be computed as: $s.e._{(jackknife)} = \sqrt{\frac{n-1}{n} \sum_{i=1}^{n} (\bar{x}_{i} - \bar{x}_{(.)})^{2}}$, where $\bar{x}_{i}$ is the correlation based on leaving out the $i$th object class and $\bar{x}_{(.)} = \frac{1}{n} \sum_{i}^{n} \bar{x}_{i}$, the mean correlation across all subsamples (of size 15).

### Category classification analyses
Model features were also used to train softmax classifiers (http://scikit-learn.org/) with L2 regularization to evaluate the degree to which category information was linearly accessible from sketches made by each group of participants. Predictions are then made for images held out from the training set, and accuracy is assessed on these held-out images. The robustness of classifier accuracy scores was determined using stratified 5-fold cross validation on 80\% train/20\% test class-balanced splits.

```{r echo=FALSE, include=FALSE}
noiseCeiling <- read.csv("vggOutputs/adultReliability.csv") %>%
  as.tibble() %>%
  gather(key = Iteration, value = corr, Iter1:Iter10)  %>%
  mutate (LayerNum = as.numeric(X)) %>%
  group_by(LayerNum) %>%
  summarize(correlation = mean(corr), SEM = sd(corr)/sqrt(10))   %>%
  mutate(cohortType="Adult-Adult")

cohortSim <- read.csv("vggOutputs/CohortSimilarity.csv") %>%
  as.tibble() %>%
  mutate(LayerNum = as.numeric(X)+1)  %>%
  gather(key = cohortType, value = correlation, c(CorrOldAdult,CorrYoungAdult)) %>%
  gather(key = cohortTypeSEM, value = SEM, c(SEMOldAdult,SEMYoungAdult)) %>%
  bind_rows(noiseCeiling) %>%
  select(-c(X, cohortTypeSEM)) %>%
  mutate(se_lower = correlation - SEM, se_upper = correlation + SEM)
```

## Results
```{r layerWise, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.label="headcam", fig.cap = "Spearman's correlation between representational dissimilarity matrices (RDMs) of drawings produced by adults vs. other adults, adults vs. older children, and between adults vs. younger children at each layer of VGG-19. Error bars represent standard error of the mean obtained by a jacknife re-sampling procedure (see Methods)."}
ggplot(cohortSim, aes(x = LayerNum, y = correlation, col=cohortType)) +
 geom_pointrange(aes(ymin = se_lower, ymax = se_upper)) +
 geom_line() +
 theme_few() +
 labs(y = "Speraman's correlation", x = "Layer Number") +
 scale_x_continuous(breaks = c(1,2,3,4,5,6,7)) +
 theme(legend.position = c(0.8, 0.2), legend.text = element_text(size=6), legend.key.size = unit(1,"line"),  legend.background = element_rect(fill=alpha('white', 0))) +
 scale_color_viridis(discrete=TRUE, begin=0, end=.8, direction = -1, "",      labels=c("Adult-Adult","OlderKids-Adults","Younger-Adults"))
```
### Layer-wise feature similarity
We first examined the featural similarities between sketches produced by adults and children at each layer of VGG-19. Overall, we found that the similarity between older children and adults' drawings increased in each subsequent layer of the network, reaching a peak in the final layers of the network (see Figure \ref{fig:layerWise}). For younger children, we found a similar pattern of results, though similarity to adult drawings was overall lower. The RDMs for the final layer of the network (where similarity was the highest; FC7) are shown in Figure \ref{fig:RSAAllCat}. 

### Classification results
We examined the degree to which these featural representations could be used to classify these sketches. We found that sketches made by younger children were classifiable 35% of the time (SD=5%), while those made by older children (7-10 years) were classifiable 51% of the time (SD=6%). While the overall performance of the classifier was relatively low compared to the human performance seen in Part 1, we still observed a relative increase in recognizability between younger and older children. Thus, these results suggest that this difference in recognizabilitystems directly perceptual feature representations. Taken together, these results suggest that children and adults are accessing somewhat similar category representations when asked to "draw a chair", and that these representations manifest in perceptual similarities in children and adults' drawings.


# General Discussion
We examined the semantic and perceptual feautres of drawings of common object categories produced by children. We found that the capacity to quickly produce drawings that communicate category information improves with age, irrespective of low-level motor covariates. We then compared children's and adult's drawings using a generic perceptual feature space dervied from deep convolutoinal neural networks; we found that children and adult drawings were most similar in the higher-level layers of a deep convolutional neural network trained to recognize objects, suggesting that children and adults drawings share many of the perceptual features useful for object recognition. 

We see two obvious future directions for this work. A first is to continue to examine how children's fine motor control influences their drawings. While drawing recognizability increased with age when accounting for the overall time spent drawing and the number of strokes, these measures only partially estimate children's motoric abilities. In future work, we plan to also measure children's ability to perform orthogonal fine motor tasks (e.g., tracing a complex shape) and to allow children longer time to complete their drawings. Second, we deliberately selected a set of words that children of all ages were likely to understand, whereas children are continuously learning about new object categories and their properties. How does this learning affect children's internal representations (and drawings) of different objects?

One possibility is that the bulk of the development change revolves around building more detailed internal representations of object categories: children may be learning the suite of visual features and object parts that tend to be diagnostic of different object categories. On this account, learning what tigers tend to look like doesn't change children's perceptual representations of cheetahs---or how they draw them. A second possibility is that learning about new categories actually changes the similarity structure of children’s visual object concepts [@goldstone2001altering]. Furthermore, as children learn about the hierarchical structure of object categories (i.e., living thing--animal--mammal--dog) and their typical properties (e.g., all mammals have four legs) this might differentially change which visual features take precedence in their internal representations. Future work that links childrens' categorization abilities with drawing behaviors will help to explore these possibilities.

Together, this work integrates novel methods to investigate children's internal representations of object categories and how they are linked to their developing perceptual, cognitive, and motor abilities. We propose that a full understanding of how we come to produce visual abstractions will help uncover the primary factors that shape how we represent our adult object representations.

\vspace{1em} \fbox{\parbox[b][][c]{7.3cm}{\centering All data and code for these analyses are available at\ \url{https://github.com/brialorelle/kiddraw}}} \vspace{1em}

# Acknowledgements
We thank Jacqueline Quirke for help with piloting and data collection. We thank  members of Stanford Language and Cognition lab. This work was partially funded by an NSF SPRF-FR (1714726) grant to BLL. We also gratefully acknowledge those who made the Google QuickDraw database available.

# References
```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in}
\setlength{\leftskip}{0.125in}
\noindent
