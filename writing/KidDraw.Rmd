---
title: "Drawings as a window into object representations in childhood"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Bria Long} \\ \texttt{bria@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Judy Fan} \\ \texttt{jefan@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Michael C. Frank } \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: 
    "The abstract should be one paragraph, indented 1/8 inch on both sides,
in 9 point font with single spacing. The heading Abstract should
be 10 point, bold, centered, with one line space below it. This
one-paragraph abstract section is required only for standard spoken
papers and standard posters (i.e., those presentations that will be
represented by six page papers in the Proceedings)."
    
keywords:
    "object representations; drawings; child development"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(knitr)
library(tidyverse)
library(assertthat)
library(ggthemes)
library(lme4)
library(langcog)
library(forcats)
library(gridExtra)
theme_set(theme_few())
```

# Introduction

Consider what one has to do in order to draw “a phone” – one needs to access a the mental representation of “a phone”, distill this into a pictoral format, and plan a sequence of motor actions to effectively convey this visual concept. Yet this is a trivial task for ordinary adults. How do we learn to so effectively produce recognizable drawings? And might drawings offer a window into how young children represent common object categories?

While drawing has been extensively studied in early childhood, a primary focus has been on when children come to treat drawings as symbols for object categories (Gardner, 1973).  And a wealth of evidence now suggests that in fact young children attribute rich meanings to their drawings. For example, children will attribute different symbolic content (e.g., “a balloon”, “a lollipop”) to very similar drawings based on what they intended to draw (Bloom & Markson, 1998). Further, children will monitor whether their drawings are adequate symbols for the things they are trying to draw, and will improve their drawings when given feedback that their drawings are not effective at communicating the identity of object (Callagan, 1999).

Far less research, however, has examined how children’s drawings reflect how children represent objects in the world around them. Indeed, drawings are a powerful way to tap internal representations of object categories, even in non-expert adults. For example, adults tend to draw objects that are small in the real-world at small visual sizes, and objects that are big in the real-world at big visual sizes (Konkle & Oliva, 2011).  Further, to be recognizable, drawings have to depict the necessary features to express a given visual concept.  This intuition is supported by recent computational work: deep neural network models of the ventral stream trained purely on photographs can also recognize drawings by non-expert adults, as drawings and photographs generated similar representations in higher-level layers of these models.  In other words, drawings capture high-level similarity relationships between object categories (Fan, Yamins, & Turk-Browne, 2015).

Here, we explore how children draw common object categories across early childhood. First, we ask if children produce more recognizable drawings as they get older after factoring out low-level covariates (e.g., the number of strokes, ink used).  Second, we examine the degree to which children’s drawings contain the perceptual features characteristic of common object categories by comparing their representations in a deep convolutional neural network. 

# Part 1: Why do children get better at drawing?
First, we examined how children across a wide range of ages produced drawings of 16 common object categories in a simple drawing game. Then, we asked naïve adults to recognize these drawings using a forced-choice recognition task.

## Methods
### Participants.  
For the drawing task, children (N = 41, M = 6.9 years, range 4-10 years) were recruited at the San Jose Children’s Discovery Museum and participated in this experiment. For the recognizability experiment, 14 adults with US IP addresses were recruited and rated all of the 268 drawings.
 
### Materials.  
We implemented a simple drawing game in HTML/Javascript using the paper.js library; this web-based experiment was run on an iPad on the floor of the museum. All code is available at www.github.com/brialorelle/kiddraw/museumdraw.
 
### Drawing Game Procedure:  
On each trial, a text cue would appear (i.e., “Can you draw a [dog]?”) that the experimenter would read out, (“What about a [dog]? Can you draw a [dog]?). Then, a drawing canvas appeared (600 x 600 pixels) and children were had 30 seconds to make a drawing before the game moved on to the next trial. After each trial, the experimenter asked the child whether they wanted to keep drawing or whether they were all done. On the first two trials of the experiment, every child was prompted to draw the same two common shapes— a circle and a triangle. These trials served to familiarize children with the drawing task and to practice using their fingers to draw.
 
### Stimuli. 
Stimuli were words referring to 16 common object categories (banana, boat, car, carrot, cat, chair, couch, cup, flower, foot, frog, ice cream, phone, rabbit, shoe, train). These categories were chosen such that they were (1) likely to be familiar to children, (2) present in the Google QuickDraw database, (3) spanned the animate/inanimate distinction and (4) intuitively spanned a wide range of difficulty (for example, flowers seem easier to draw than couches).
 
###Recognizability Task:  
14 naïve adults assessed the recognizability of all of the 286 drawings produced by these children. On each trial, participants saw a drawing, and were asked “What does this look like?”, and responded by typing into a text box; participants could then choose between 21 possible answers. 16 of these possible answers were the original object categories; however, we also included five additional foil items (bean, arm, person, rock, and “cannot tell at all”). All drawings were presented in a random order, and participants were not informed that these drawings were produced by children or the context in which they were produced. An answer was scored as “correct” if adults were able to correctly guess the object category that children were cued with.

```{r echo=FALSE, include=FALSE}
## Load data and do basic preprocessing.

## Read in data outputs from python - stroke numbers, intensity, bounding boc, etc.
# get rid of drwaings without age - these were when we were testing the interface.
# make new variable name with image name for joining with recognition data
d <- read_csv("e1-preprocessedData/museumdraw_E1c_imageData.csv") %>%
  filter(!is.na(age)) %>%
  mutate(imNameShort = paste0(category, '_sketch', '_', age,'_', session_id, '.png'))

## Read in data outputs from turk data - true/false recognition with 21AFC
r <- read.csv("e1-preprocessedData/museumdraw_E1c_recognitionData.csv") %>%
  as.tibble()

## check we have the right lengths
assert_that(length(d$session_id)==length(unique(r$imageName)))

# add special column for when people selected "can't tell at all" during ratings; not separated out in current analyses 
r$cantTell=(r$rating=="cannott tell at all")

## Get the percent recognized for each drawing
corbyItem <- r %>%
  group_by(imNameShort) %>%
  summarize(meanCorrect = mean(correct), 
            propCantTell = mean(cantTell))

## Join the two datasets together
joint=left_join(d,corbyItem) %>%
  mutate(session_id = factor(session_id), 
         category = factor(category))

## for use below with glmer analyses
joinedRatings <- left_join(r,d)
joinedRatings$session_id<-factor(joinedRatings$session_id)

## percent correct by age
ageCorrOut<-joint %>%
  group_by(age) %>%
  summarize(count = n(), 
            meanCorrect = mean(meanCorrect), 
            propCantTell = mean(propCantTell)) 
```

```{r exampleDrawings, fig.env = "figure", fig.pos = "h", out.width="100%", fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Example drawings made by children ages 4-10 of several object  categories."}
img <- png::readPNG("figs/drawings.png")
grid::grid.raster(img)
```
### Low-level covariates. 
The use of a digital interface for drawing allowed us to quickly and easily assess the contribution of several low-level factors that may co-vary with drawing ability. For each drawing, we thus quantified the amount of time spend drawings, the number of strokes used, and the overall intensity (e.g., amount of ink). Descriptives plots describing the output of these variables can be seen in (see Figure \ref{fig:covDescriptives}, left). 
```{r covDescriptives, fig.env="figure*", fig.height=2, fig.width=6, fig.pos = "h", fig.align = "center", fig.cap = "R plot" }
p1<-ggplot(joint, aes(x = num_strokes, y=meanCorrect, color=age)) +
  geom_jitter(alpha=.5) +
  geom_smooth(aes(group = 1), method="loess",span=3, alpha=.1) +
  theme(legend.position="left", aspect.ratio=1) +
  labs(y = "Prop. recognized", x = "Number of strokes used") 

p2<-ggplot(joint, aes(x = mean_intensity, y=meanCorrect, color=age)) +
  geom_jitter(alpha=.5) +
  geom_smooth(aes(group = 1), method="loess",span=3, alpha=.1) +
  theme(legend.position="none", aspect.ratio=1) +
  labs(y = "Prop. recognized", x = "Amount of ink used") 

p3<-ggplot(joint, aes(x = draw_duration, y=meanCorrect, color=age)) +
  geom_jitter(alpha=.5) +
  geom_smooth(aes(group = 1), method="loess",span=3, alpha=.1) +
  theme(legend.position="none", aspect.ratio=1) +
  labs(y = "Prop. recognized", x = "Time spent drawing") 

grid.arrange(p1,p2,p3, nrow = 1)

```
###  GLMM procedure. 
We aimed to assess whether children’s ability to produce recognizable drawings increased with age, independent of low-level covariates. To do so, we used a generalized logistic mixed effect model, with age, drawing duration, amount of ink used, and number of strokes as fixed effects, and with random effects for each individual child drawer and object category. 

## Results
``` {r include=FALSE, echo=FALSE}
## GLMM procedure
modelOut <- glmer(correct ~ age + (1 | session_id) + (1 | category), 
      data = joinedRatings,  
      family = "binomial")

mod_covariates <- glmer(correct ~ scale(age) + 
                          scale(draw_duration) + 
                          scale(mean_intensity) + 
                          scale(num_strokes) +
                        (1|session_id) + 
                        (1|category), 
      data = joinedRatings,  
      family = "binomial")

mod_covariates_2 <- glmer(correct ~ (scale(age) + 
                          scale(draw_duration) + 
                          scale(num_strokes))^2 +
                        (1|session_id) + 
                        (1|category), 
      data = joinedRatings,  
      family = "binomial")

modelOut=summary(mod_covariates)
```

First, we observed that some items were much easier to draw than others. For example, children of all ages produced drawings of cats that were readily recognizable as "cats", but few children of any age produced drawings that were recognizable as "shoes" (see Figure \ref{fig:recognizabilityByItem}). Howver, almost all items also saw an increase in recognizability with the age of the drawer. Across all items, we saw that the proportin of drawings recognized increased steadily with age (% drawings recognized; chance = 4.8%; $M_{4yrs}$ = `r format(ageCorrOut$meanCorrect[1]*100,digits=2)`%, $M_{5yrs}$ = `r format(ageCorrOut$meanCorrect[2]*100,digits=2)`%, $M_{6yrs}$ = `r format(ageCorrOut$meanCorrect[3]*100,digits=2)`%, $M_{7yrs}$ = `r format(ageCorrOut$meanCorrect[4]*100,digits=2)`%, $M_{8yrs}$ = `r format(ageCorrOut$meanCorrect[5]*100,digits=2)`%, $M_{9yrs}$ = `r format(ageCorrOut$meanCorrect[6]*100,digits=2)`%, $M_{10yrs}$ = `r format(ageCorrOut$meanCorrect[7]*100,digits=2)`%).

Next, we asked whether this relationsip would persist when we controlled for these item effects and the number of strokes, amount of ink used, and the time spent drawing. In other words, is this increase in recognizability due to an increase in expressive power, or simply due to the fact that older children may have put more effort into their drawings?
Our generalized logistic mixed-effect model revealed that the recognizability of drawings increased reliably with age (b = `r format(modelOut$coefficients[2,1],digits=2)`, SE = `r format(modelOut$coefficients[2,2],digits=2) `, Z = `r format(modelOut$coefficients[2,3],digits=2)`), accounting for variation across object categories and individual children and controlling for several low-level covariates — the amount of time spent drawing, the number of strokes, and total ink used. All model coefficients can be seen in Table \ref{table:modelCoefficients}


```{r recognizabilityByItem, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=4, fig.height=4, fig.cap = "Proportion of drawings recognized for object category, sorted from hardest to easiest items. Error bars represent non-parametric 95 percent confidence intervals, estimated using the langcog r package." }

ms <- joint %>%
  mutate(age_group = cut(age, c(3.9, 6, 8, 10.1), labels = c("4-6","6-8","8-10"))) %>%
  group_by(category, age_group) %>%
  multi_boot_standard(col = "meanCorrect")  %>%
  ungroup %>%
  mutate(category = fct_reorder(category, mean))

ggplot(ms, aes(x = category, y = mean, col = age_group)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  coord_flip() +
  labs(y = "Proportion recognized", x = "Object category") 
  
```

```{r modelCoefficients, results="asis"}
tab1 <- xtable::xtable(summary(mod_covariates)$coef, digits=3, 
                      caption = "Model coefficients of a GLMM predicting the recognziability of each   drawing.")

print(tab1, type="latex", comment = F, table.placement = "H")
```

 
# Part 2: How similar are children's and adults drawings?  

However, these recognizability ratings may underestimate the perceptual content depicted in children’s drawings. For example, children may not be able to depict the visual differences between a bunny and a frog, but they still may capture many of the essential perceptual features needed to depict an animal. Indeed, this trend was somewhat evident in the confusion matrices from beforehand: drawings of cats were most confused with frogs and bunnies (and not couches or chairs). Here, we turn to deep neural network models of object recognition to quantify the similarities between children’s and adults drawings, asking how similar they are in terms of feature similarity at each progressively complex layer of a deep convolutional neural network. While younger and older children’s drawings may differ in the high-level visual features needed to recognize the depicted object at the basic-level (e.g., to ears needed to recognize the sketch as a “bunny”), might they instead be more similar at mid-level layers of a deep convolutional neural network?

For this analyses, we aimed to collect a larger sample of drawings using the same methodology, this time sampling from both the previously used categories as well as a new selection of 22 categories (see Stimuli) allowing us to span superordinate category distinctions.  With this larger sample of drawings, we thus examine the degree to which children and adults drawings resemble each other in a deep convolutional neural network, specifically VGG-19 (cite).

## Methods

### Participants
Participants included those who participated in the first round of data collection, used in Experiment 1, as well as an additional XX children (average age = XX years), again recruited from the floor of the San Jose Children’s Discovery Museum. Overall, this yielded an additional XX drawings (excluding practice trials), where older children (ages 7-9  years) produced XX drawings, and younger children (3-6 years) produced XX drawings, yielding a total of XX drawings for analyses (older: XX drawings, younger: XX drawings).

### Stimuli
For this second round of data collection, we expanded our set of categories to include equal numbers of vehicles, furniture, small objects, food items, mammals, and non-mammals (new items: airplane, bus, bike, piano, table, door, bed, fork, keys, hat, apple, cookie, mushroom, horse, dog, sheep, bear, fish, bird, spider, shark, duck).

### Adult drawings
We obtained a sample of adult drawings from the Google QuickDraw database (https://quickdraw.withgoogle.com/data). Specifically, we randomly sampled 1000 images for each category.

### CNN Features  
We used a standard, pre-trained implementation of VGG-19 (cite VGG-19) to extract features in response to all sketches at each layer of the network, including the first five convolutional layers (C1-C5) as well as the two fully-connected layers (FC6 and FC7). Features were normalized within each layer across all sketches and then averaged within each category (e.g., “cat”, “rabbit”). This yielded a vector corresponding to the number of features in each layer for all 38 of the drawn categories in younger children, older children, and adults. 

### Representational Similarity Analyses 
To construct similarity matrixes for each of the categories, the feature vectors for each categories were correlated with all other categories. The upper diagonal of the resulting, symmetric correlation matrix (shown in Figure XX) were then correlated (Spearman’s r) across age groups at within each layer of VGG-19. 
## Results
First, we analyzed the full set of drawings, and found that both younger children and older children generated sketches that moderately resembled those of adults’ in higher-level layers (FC7) of a deep convolutional neural network trained to recognize objects. Younger children’s category similarities moderately resembled adults (Spearman’s r=XX), while older children’s category similarities were significantly more structured and adult-like (Spearman’s r=XX). This first analyses thus supports the results of Experiment 1, where we found that the recognizability of the sketches improved with age, irrespective of low-level covariates.
Second, we also analyzed a subset of the categories (14)  in which we had at least five drawings from both younger and older children. 

# General Discussion

These results suggest that the capacity to quickly produce graphical representations that communicate object category information is highly developed by middle childhood.

These results also suggest that the ability to produce visual concepts is highly developed by middle childhood. In under 30 seconds, even 6-year-olds produced drawings that were on average %% recognizable in our 21AFC task.

# Acknowledgements

Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
