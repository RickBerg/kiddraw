---
title: "Drawings as a window into object representations in childhood"
bibliography: kiddraw.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Bria Long} \\ \texttt{bria@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Judy Fan} \\ \texttt{jefan@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Michael C. Frank } \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: 
    "A few well placed strokes can convey the identity of a person, object, or place. How do we become so skilled at creating these graphical abstractions?  Here, we take a developmental appraoch to question by asking children (age range 3-10 years) to draw a wide range of objects (e.g. 'can you draw a chair?'). First, we analyze the degree to which children were able to depict recognizable exemplars of these categories in under 30 seconds. We find that children become increasingly skilled at producing recognizable drawings, even when controlling for low-level covariates (the amount of ink used, the time spent drawing, and the number of strokes). Second, we analyze the degree to which children's drawings share perceptual features with those produced by adults by analyzing their featural similarity at each layer of deep convolutional neural network (VGG-19). Overall, we find that older children's drawings were most similar to adults in higher-levle layers of the network, while younger childrens drawings were most similar to adults in mid-level layers of the network. Thus, these results suggest that the ability to produce abstract graphical represnetations of objects is highly developed by middle childhood, but that even the most primitive drawings made by children contain featural similarities to the objects they are trying to depict. Broadly, these results
point towards drawing as a promising avenue for investigating object representations in childhood."
  
keywords:
    "object representations; drawings; child development"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(knitr)
library(tidyverse)
library(assertthat)
library(ggthemes)
library(lme4)
library(langcog)
library(forcats)
library(gridExtra)
theme_set(theme_few())
```

# Introduction

Consider what one has to do in order to draw “a phone” – one needs to access a the mental representation of “a phone”, distill this into a pictoral format, and plan a sequence of motor actions to effectively convey this visual concept. Yet this is a trivial task for ordinary adults. How do we learn to so effectively produce recognizable drawings? And might drawings offer a window into how young children represent common object categories?

While drawing has been extensively studied in early childhood, a primary focus has been on when children come to treat drawings as symbols for object categories [@gardner1994arts].  And a wealth of evidence now suggests that in fact young children attribute rich meanings to their drawings. For example, children will attribute different symbolic content (e.g., “a balloon”, “a lollipop”) to very similar drawings based on what they intended to draw [@bloom1998intention]. Further, children will monitor whether their drawings are adequate symbols for the things they are trying to draw, and will improve their drawings when given feedback that their drawings are not effective at communicating the identity of object [@callaghan1999early].

Far less research, however, has examined how children’s drawings reflect how children represent objects in the world around them. Indeed, drawings are a powerful way to tap internal representations of object categories, even in non-expert adults. For example, adults tend to draw objects that are small in the real-world at small visual sizes, and objects that are big in the real-world at big visual sizes (@konkle2011canonical). Further, to be recognizable, drawings have to depict the necessary features to express a given visual concept.  This intuition is supported by recent computational work: deep neural network models of the ventral stream trained purely on photographs can also recognize drawings by non-expert adults, as drawings and photographs generated similar representations in higher-level layers of these models.  In other words, drawings capture high-level similarity relationships between object categories [@fan2015common].

Here, we explore how children draw common object categories across early childhood. First, we ask if children produce more recognizable drawings as they get older after factoring out low-level covariates.  Second, we examine the degree to which children produce drawings that contain similar perceptual features characteristic of common object categories by comparing their children and adult's drawings at each layer of a deep convolutional neural network. 

# Part 1: Why do children get better at drawing?
First, children across a wide range of ages produced drawings of 16 common object categories in a simple drawing game. Then, naïve adults attempted to recognize these drawings in a forced-choice recognition task.

## Methods
### Participants
For the drawing task, children (N = 41, M = 6.9 years, range 4-10 years) were recruited at the San Jose Children’s Discovery Museum and participated in this experiment. For the recognizability experiment, 14 adults with US IP addresses were recruited and rated all of the 268 drawings.
 
### Materials
We implemented a simple drawing game in HTML/Javascript using the paper.js library; this web-based experiment was run on an iPad on the floor of the museum. All code is available at www.github.com/brialorelle/kiddraw/museumdraw.
 
### Drawing Game Procedure 
On each trial, a text cue would appear (i.e., “Can you draw a [dog]?”) that the experimenter would read out, (“What about a [dog]? Can you draw a [dog]?). Then, a drawing canvas appeared (600 x 600 pixels) and children were had 30 seconds to make a drawing before the game moved on to the next trial. After each trial, the experimenter asked the child whether they wanted to keep drawing or whether they were all done. On the first two trials of the experiment, every child was prompted to draw the same two common shapes—-a circle and a triangle. These trials served to familiarize children with the drawing task and to practice using their fingers to draw.
 
### Stimuli
Stimuli were words referring to 16 common object categories (banana, boat, car, carrot, cat, chair, couch, cup, flower, foot, frog, ice cream, phone, rabbit, shoe, train). These categories were chosen such that they were (1) likely to be familiar to children, (2) present in the Google QuickDraw database, (3) spanned the animate/inanimate distinction and (4) intuitively spanned a wide range of difficulty (for example, flowers seem easier to draw than couches).
 
###Recognizability Task
14 naïve adults assessed the recognizability of all of the 286 drawings produced by these children. On each trial, participants saw a drawing, and were asked “What does this look like?”, and responded by typing into a text box; participants could then choose between 21 possible answers. 16 of these possible answers were the original object categories; however, we also included five additional foil items (bean, arm, person, rock, and “cannot tell at all”). All drawings were presented in a random order, and participants were not informed that these drawings were produced by children or the context in which they were produced. An answer was scored as “correct” if adults were able to correctly guess the object category that children were cued with.

```{r echo=FALSE, include=FALSE}
## Load data and do basic preprocessing.

## Read in data outputs from python - stroke numbers, intensity, bounding boc, etc.
# get rid of drwaings without age - these were when we were testing the interface.
# make new variable name with image name for joining with recognition data
d <- read_csv("e1-preprocessedData/museumdraw_E1c_imageData.csv") %>%
  filter(!is.na(age)) %>%
  mutate(imNameShort = paste0(category, '_sketch', '_', age,'_', session_id, '.png'))

## Read in data outputs from turk data - true/false recognition with 21AFC
r <- read.csv("e1-preprocessedData/museumdraw_E1c_recognitionData.csv") %>%
  as.tibble()

## check we have the right lengths
assert_that(length(d$session_id)==length(unique(r$imageName)))

# add special column for when people selected "can't tell at all" during ratings; not separated out in current analyses 
r$cantTell=(r$rating=="cannott tell at all")

## Get the percent recognized for each drawing
corbyItem <- r %>%
  group_by(imNameShort) %>%
  summarize(meanCorrect = mean(correct), 
            propCantTell = mean(cantTell))

## Join the two datasets together
joint=left_join(d,corbyItem) %>%
  mutate(session_id = factor(session_id), 
         category = factor(category))

## for use below with glmer analyses
joinedRatings <- left_join(r,d)
joinedRatings$session_id<-factor(joinedRatings$session_id)

## percent correct by age
ageCorrOut<-joint %>%
  group_by(age) %>%
  summarize(count = n(), 
            meanCorrect = mean(meanCorrect), 
            propCantTell = mean(propCantTell)) 

```

```{r exampleDrawings, fig.env = "figure*", fig.pos = "h", out.width="100%", fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Example drawings made by children ages 4-10 of several object  categories."}
img <- png::readPNG("figs/drawings.png")
grid::grid.raster(img)
```
### Low-level covariates. 
The use of a digital interface for drawing allowed us to quickly and easily assess the contribution of several low-level factors that may co-vary with drawing ability. For each drawing, we thus quantified the amount of time spend drawing, the number of strokes used, and the overall intensity of the drawing(e.g., amount of ink). Descriptives plots describing the output of these variables can be seen in (see Figure \ref{fig:covDescriptives}, left). 
```{r covDescriptives, fig.env="figure*", fig.height=3, fig.width=7, fig.pos = "h", fig.align = "center", fig.cap = "The proportion of adults who recognized each drawing is plotted as a function of the number of strokes, amount of ink used, and the time spent creating each drawing. Each dot represents an individual drawing; dots are colored by the age of the drawer." }
ms <- joint %>%
  mutate(age_group = cut(age, c(3.9, 6, 8, 10.1), labels = c("4-6","6-8","8-10"))) 

p1<-ggplot(ms, aes(x = num_strokes, y=meanCorrect, col=age_group)) +
  geom_jitter(alpha=.5) +
  geom_smooth(aes(group = 1), method="loess",span=3, alpha=.1, color="grey") +
  theme(legend.direction = "horizontal") +
  ylim(0, 1) +
  scale_color_brewer(palette="Paired", "Age group") +
  labs(y = "Prop. recognized", x = "Number of strokes") 

p2<-ggplot(ms, aes(x = mean_intensity, y=meanCorrect, col=age_group)) +
  geom_jitter(alpha=.5) +
  geom_smooth(aes(group = 1), method="loess",span=3, alpha=.1, color="grey") +
  scale_color_brewer(palette="Paired", "Age group") +
  labs(y = "", x = "Amount of 'ink' used") 

p3<-ggplot(ms, aes(x = draw_duration, y=meanCorrect, col=age_group)) +
  geom_jitter(alpha=.5) +
  geom_smooth(aes(group = 1), method="loess",span=3, alpha=.1, color="grey") +
  scale_color_brewer(palette="Paired", "Age group") +
  labs(y = "", x = "Time spent drawing") 

g_legend<-function(a.gplot){
  tmp <- ggplot_gtable(ggplot_build(a.gplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)}

mylegend<-g_legend(p1)
grid.arrange(arrangeGrob(p1 + theme(legend.position="none"),
                         p2 + theme(legend.position="none"),
                         p3 + theme(legend.position="none"),
                         nrow=1),
             mylegend, nrow=2,heights=c(12, 2))
```
###  GLMM procedure. 
We aimed to assess whether children’s ability to produce recognizable drawings increased with age, independent of low-level covariates. To do so, we used a generalized logistic mixed effect model, with age, drawing duration, amount of ink used, and number of strokes as fixed effects, and with random effects for each individual child drawer and object category. 

## Results
``` {r include=FALSE, echo=FALSE}
## GLMM procedure
modelOut <- glmer(correct ~ age + (1 | session_id) + (1 | category), 
      data = joinedRatings,  
      family = "binomial")

mod_covariates <- glmer(correct ~ scale(age) + 
                          scale(draw_duration) + 
                          scale(mean_intensity) + 
                          scale(num_strokes) +
                        (1|session_id) + 
                        (1|category), 
      data = joinedRatings,  
      family = "binomial")

mod_covariates_2 <- glmer(correct ~ (scale(age) + 
                          scale(draw_duration) + 
                          scale(num_strokes))^2 +
                        (1|session_id) + 
                        (1|category), 
      data = joinedRatings,  
      family = "binomial")

modelOut=summary(mod_covariates)
modelOut_Int=summary(mod_covariates_2)
```

First, we observed that some items were much easier to draw than others. For example, children of all ages produced drawings of cats that were readily recognizable as "cats", but few children of any age produced drawings that were recognizable as "shoes" (see Figure \ref{fig:recognizabilityByItem}). Howver, almost all items also saw an increase in recognizability with the age of the drawer. Across all items, the proportion of drawings recognized increased steadily with age (% drawings recognized; chance = 4.8%; $M_{4yrs}$ = `r format(ageCorrOut$meanCorrect[1]*100,digits=2)`%, $M_{5yrs}$ = `r format(ageCorrOut$meanCorrect[2]*100,digits=2)`%, $M_{6yrs}$ = `r format(ageCorrOut$meanCorrect[3]*100,digits=2)`%, $M_{7yrs}$ = `r format(ageCorrOut$meanCorrect[4]*100,digits=2)`%, $M_{8yrs}$ = `r format(ageCorrOut$meanCorrect[5]*100,digits=2)`%, $M_{9yrs}$ = `r format(ageCorrOut$meanCorrect[6]*100,digits=2)`%, $M_{10yrs}$ = `r format(ageCorrOut$meanCorrect[7]*100,digits=2)`%).

Next, we asked whether this relationsip persists when we control for low-level covariates: the number of strokes, amount of ink used, and the time spent drawing. In other words, is this increase in recognizability due to an increase in expressive power, or simply due to the fact that older children may have put more effort into their drawings? Our generalized logistic mixed-effect model revealed that the recognizability of drawings increased reliably with when controlling for these low-level covariates — the amount of time spent drawing, the number of strokes, and total ink used (b = `r format(modelOut$coefficients[2,1],digits=2)`, SE = `r format(modelOut$coefficients[2,2],digits=2) `, Z = `r format(modelOut$coefficients[2,3],digits=2)`), and accounting for variation across object categories and individual children. All model coefficients can be seen in Table \ref{table:modelCoefficients}. Adding interaction terms between age and these low-level covariates did little to decrease the effect of age on recognizability (b = `r format(modelOut_Int$coefficients[2,1],digits=2)`, SE = `r format(modelOut_Int$coefficients[2,2],digits=2) `, Z = `r format(modelOut_Int$coefficients[2,3],digits=2)`). 

Thus, these results suggest that the ability to quickly produce graphical representations of object categories increases with age, independently of low-level covariates. Further, these results sugest that this ability is highly developed by middle childhood, plateauing for these object cateogries around age 6-7.

```{r recognizabilityByItem, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=4, fig.height=4, fig.cap = "Proportion of drawings recognized for object category, sorted from hardest to easiest items. Error bars represent non-parametric 95 percent confidence intervals, estimated using the langcog r package." }

ms <- joint %>%
   mutate(age_group = cut(age, c(3.9, 6, 8, 10.1), labels = c("4-6","6-8","8-10"))) %>%
  # mutate(age_group = cut(age, c(3.9, 6, 10.1), labels = c("3-6","7-10"))) %>%
  group_by(category, age_group) %>%
  multi_boot_standard(col = "meanCorrect")  %>%
  ungroup %>%
  mutate(category = fct_reorder(category, mean))

ggplot(ms, aes(x = category, y = mean, col = age_group)) + 
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) +
  coord_flip() +
  labs(y = "Proportion recognized", x = "Object category") +
  scale_color_brewer(palette="Paired", "Age group")
  
```

<!-- ```{r modelCoefficients, results="asis"} -->
<!-- tab1 <- xtable::xtable(summary(mod_covariates)$coef, digits=3,  -->
<!--                       caption = "Model coefficients of a GLMM predicting the recognziability of each  drawing.") -->

<!-- print(tab1, type="latex", comment = F, table.placement = "H") -->
<!-- ``` -->

\begin{table}[H]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & z value & Pr($>$$|$z$|$) \\ 
  \hline
(Intercept) & 0.861 & 0.321 & 2.680 & 0.007 \\ 
  Age & 0.956 & 0.174 & 5.497 & 0.000 \\ 
  Drawing time & 0.338 & 0.109 & 3.105 & 0.002 \\ 
  Amount of ink & 0.014 & 0.080 & 0.179 & 0.858 \\ 
  Num. strokes & -0.289 & 0.098 & -2.959 & 0.003 \\ 
   \hline
\end{tabular}
\caption{Model coefficients of a GLMM predicting the recognziability of each  drawing.} 
\end{table}
 
# Part 2: How similar are children's and adults drawings?  

To what degree are children's drawings similar to those of adults? While younger children often produced drawings that were unrecognizable at the basic-level,  these recognizability ratings may underestimate the perceptual content depicted in children’s drawings. For example, children may not be able to depict the visual differences between a bunny and a frog, but they still may capture many of the essential perceptual features needed to depict an animal. Here, we turn to deep neural network models of object recognition to quantify the similarities between children’s and adults drawings, asking how similar they are in terms of feature similarity at each progressively complex layer of a deep convolutional neural network. In other words, how similar are children and adult's drawings (and why)?

For this analyses, we collected a larger sample of drawings using the same methodology, this time sampling from both the previously used categories as well as a new selection of 22 categories (see Stimuli) allowing us to span superordinate category distinctions.  With this larger sample of drawings, we thus examine the degree to which children and adults drawings resemble each other in a deep convolutional neural network, specifically VGG-19.

## Methods

### Participants
Participants included those who participated in the first round of data collection, used in Experiment 1, as well as an additional 37 children, again recruited from the floor of the San Jose Children’s Discovery Museum. Overall, this yielded an additional 98 drawings (excluding practice trials). This left us with a total of 177 drawings made by

### Stimuli
For this second round of data collection, we expanded our set of categories to include equal numbers of vehicles, furniture, small objects, food items, mammals, and non-mammals (new items: airplane, bus, bike, piano, table, door, bed, fork, keys, hat, apple, cookie, mushroom, horse, dog, sheep, bear, fish, bird, spider, shark, duck).

### Adult drawings
We obtained a sample of adult drawings from the Google QuickDraw database (https://quickdraw.withgoogle.com/data). Specifically, we randomly sampled 1000 images for each category.

### CNN Features  
We used a standard, pre-trained implementation of VGG-19 (cite VGG-19) to extract features in response to all sketches at each layer of the network, including the first five convolutional layers (C1-C5) as well as the two fully-connected layers (FC6 and FC7). Features were normalized within each layer across all sketches and then averaged within each category (e.g., “cat”, “rabbit”). This yielded a vector corresponding to the number of features in each layer for all 38 of the drawn categories in younger children, older children, and adults. 

### Representational Similarity Analyses 
To construct similarity matrixes for each of the categories, the feature vectors for each categories were correlated with all other categories. The upper diagonal of the resulting, symmetric correlation matrix (shown in Figure \ref{fig:RSAAllCat}) were then correlated (Spearman’s r) across age groups at within each layer of VGG-19.  Second, we performed this same analysis in a subset of shared categories categories (14)  in which we had at least three drawings from both younger and older children.  For visualization purposes, we ordered object categories according to their naturally occuring similarity clusters in the data using a variant of affine propagation  ^^^ JUDY INSERT DETAILS HERE ^^^

### Category classification analyses
^^^ JUDY INSERT DETAILS HERE ^^^

```{r RSAAllCat, fig.env = "figure*", fig.pos = "h", fig.align='center', fig.width=7, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Category correlation matrixes in the highest layer of VGG-19 (FC7) for drawings made by younger children ( 3-6 years of age), older children (7-10 years of age), and adults (Google QuickDraw database). Each square in one of these matrixes represnets the correlation between two categories (e.g., chair and couch) in this layer of the network; lighter colors indicate higher correlations. Data-driven analyses were used to group the categories to reveal the similarity clusters. "}
img <- png::readPNG("figs/RSA_SubsetCategories.png")
grid::grid.raster(img)
```
## Results
We first compared the category similarities in the highest layer of the network (FC7). Overall, we found that 3-5 year-old's drawings had category similarities that only weakly resembled adults (Spearman’s r=.20), while 5-7 year olds had more apparent category similarities (Spearman's r=.53), and 8-10 year olds had the highest similarities  (Spearman’s r=.68), see Figure \ref{fig:RSAAllCat}).  We found the same pattern of results when only anlayzing the smaller subset of categories, finding that younger children’s category similarities moderately resembled adults (Spearman’s r=XX), while older children’s category similarities were significantly more structured and adult-like (Spearman’s r=.67). Similarily, a simple linear classifier applied to the outputs of the final layer of VGG-19 could classify sketches made by older children at XX%, and sketches made by younger children XX% (adults=XX% of the time). 

Next, we examined the featural similarities between sketches produced by adults and children at each layer of VGG-19, examining about the layer-wise emergence of these similarites. For this analysis, we restricted our comparisons to categogries for which we had three or more drawings per category per age group. Overall, we found that the similarity between children and adults' drawings increased in each subsequent layer of the network, reaching a peak in the final layers of the network (see Figure \ref{fig:layerWise}; Spearman's r values, Layer 1=0.215, Layer 2=0.206,Layer 3=0.249,Layer 4=0.435,Layer 5=0.637,Layer 6=0.696, Layer 7=0.72). For younger children, we found a similar pattern of results, though similarity to adult drawings was overall lower (Spearman's r values, Layer 1=0.074,  Layer 2=0.001,  Layer 3=0.04 ,  Layer 4=0.119,  Layer 5=0.368,  Layer 6=0.464, Layer 7=0.49).  For older children. We again found the same pattern of results when when analyzed only a subset of the categories with a high number of drawings (younger children, peak at layer XX, Spearman's r=XX; older children, peak at layer XX, Spearman's r=XX). 
had fewer low-level features in commons with adults' drawings than older children's drawings, suggesting that they in fact still contained mid and high-level perceptual feature simialrities. 

Taken together, these results suggest that children and adults are accessing similar category representations to perform these drawing tasks that manifest in perceptual similarities between adults and children, even in the most primitive drawings.


```{r layerWise, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=3, fig.height=4, set.cap.width=T, num.cols.cap=1, fig.label="headcam", fig.cap = "Between-cohort similarity "}
img <- png::readPNG("figs/btw_cohort_similarity-2.png")
grid::grid.raster(img)
```

# General Discussion
Overal, we found that the capacity to quickly produce graphical representations that communicate object category information is highly developed by middle childhood. Children produced drawings that were recognizable by adults in under 30 seconds with only the few "strokes" of a pen. This work also points toward high-level similarities between children and adults representations of object categories. 

An obvious future direction for this work is to understand the contribution of children's motor  abilities in producing graphical representations. In other words, to what degree are drawings made by older children more recognizable simply because older children have better fine motor control?  Children certianly practice drawing objects—-both on their own and in structured settings (e.g., art classes)--and this practice invariably plays a role in the fidelity of the drawings that children can produce. In the future, we plan to measure children's fine motor control on an orthogonal task (e.g., tracing a complex shape) to begin to understand how this factor influences the recognizability of the drawings that children produce. 

Ultimately, we seek to understand the degree to which changes in children's drawings of objects reflect changes in children's representations of object categories. In other words, one possibility is that children's internal represntaitons of "rabbit", "chair", and "couch" are becoming more detailed as they grow older, and that it is these more detailed representations that, in turn, feed into their rapid drawings of these object categories. Throughout childhood, children certainly acquire a wealth of experience with the objects in the world around them, and this experience likely helps build more detailed internal representations of the categories.  If this is the case, then children's abilities to depict certain object categories may patterns with their object categorization errors. For example, are older (vs. younger) children both better able to draw cats vs. rabbits and better able to distinguish between cats vs. rabbits?  Future work that links childrens perceptual and categorization abilities with their production behaviors may begin to answet this question.

In sum, this work begings a developmental project examining childrens internal object representaitons using a simple production task--drawing. An understanding of how we form simple visual abstractions of the objects in our everyday world is likely to help uncover the building blocks of our category representations.

# Acknowledgements
We gratefully acknowledge those who made the Google QuickDraw database avaliable. This work was funded by a ## to Judy Fan, and a ### to Michael C. Frank, and a ### to Bria Long.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
